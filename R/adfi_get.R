# WARNING - Generated by {fusen} from dev/flat_teaching.Rmd: do not edit by hand

#' Comprehensive Average Daily Feed Intake (ADFI) Calculation Pipeline
#'
#' @description
#' Integrated system for calculating porcine average daily feed intake (ADFI) 
#' through error detection, correction, and imputation. Processes and calibrates
#' feed intake data from feeding stations.
#'
#' @param data A data.table or data.frame containing feed intake records
#' @param adg_res Result list from adg_get() function containing ADG information
#' @param station_type Character. Type of feeding station: "nedap" or "fire". 
#'        If NULL, will auto-detect from data structure.
#' @param quiet Logical. If TRUE, suppresses messages and warnings. Default: FALSE.
#' 
#' @details
#' **Processing Workflow**:
#' \enumerate{
#'   \item **Error Detection**:
#'     \itemize{
#'       \item Generates error variables based on station type
#'       \item Identifies feed intake anomalies through predefined thresholds
#'       \item Flags problematic records for correction
#'     }
#'     
#'   \item **Error Correction**:
#'     \itemize{
#'       \item Calculates error proportions by animal and day
#'       \item Separates valid vs. error-containing intake records
#'       \item Applies LMM modeling for calibration of error feeds
#'     }
#'     
#'   \item **ADFI Calculation**:
#'     \itemize{
#'       \item Combines corrected and valid intake records
#'       \item Performs temporal imputation for missing data points
#'       \item Calculates average daily feed intake metrics
#'     }
#' }
#'
#' **Supported Station Types**:
#' \itemize{
#'   \item **NEDAP**: Processes responder, location, date, and state-based data
#'   \item **FIRE**: Processes entrancetime/exittime and weight-based feeding data
#' }
#'
#' @return A `list` containing:
#' \itemize{
#'   \item `adfi_info`: 
#'     \itemize{
#'       \item Aggregated feed intake metrics data.table
#'       \item Contains columns: `responder`, `location`, `origin_dfi`, `corrected_dfi`
#'     }
#'   \item `adfi_data`:
#'     \itemize{
#'       \item Detailed daily feed intake data.table
#'       \item Contains columns: `responder`, `location`, `date`, `seq_days`, 
#'             `origin_dfi`, `corrected_dfi`, `state`
#'       \item State indicators: "org" (original), "corr" (corrected), 
#'             "iii" (interpolated), "elrp" (extrapolated)
#'     }
#' }
#'
#' @seealso
#' \itemize{
#'   \item [adg_get()] For average daily gain calculation
#' }
#'
#' @importFrom data.table := .N .SD setnames
#' @importFrom purrr map map2
#' @importFrom recipes recipe update_role step_zv step_corr step_scale prep juice
#' @importFrom zoo na.approx
#' @importFrom stats reformulate formula
#' @importFrom cAIC4 stepcAIC
#' @export
#' @examples
#' result_nedap <- preprocess_data(data = mintyr::nedap, station_type = "nedap", quiet = TRUE)
#' adg_result <- adg_get(result_nedap, my_break = c(30,120), quiet = TRUE)
#' adfi_result <- adfi_get(result_nedap, adg_result, quiet = TRUE)
#' head(adfi_result$adfi_info)
#' head(adfi_result$adfi_data)
adfi_get <- function(data,
                     adg_res,
                     station_type = NULL,
                     quiet = FALSE) {


  # Define process function based on quiet parameter
  process_fn <- if (quiet) {
    function(expr) suppressMessages(suppressWarnings(expr))
  } else {
    function(expr) expr
  }


  # Main processing logic wrapped in process_fn
  process_fn({
    tryCatch({
      # prepare dfi data
      prepare_dfi_data <- data |>
        create_error_types(station_type = station_type) |>
        create_error_prop()

      # prepare scaled data
      scale_data <- prepare_scale_data(right_dfi = prepare_dfi_data$right_dfi,
                                       error_prop = prepare_dfi_data$error_prop_result,
                                       adg_info = adg_res$adg_info,
                                       adg_data = adg_res$adg_data)


      # correct adfi
      dfi_correct <- train_lmm_model(data = scale_data$processed_data) |>
        merge_lmm_predictions(error_dfi = prepare_dfi_data$error_dfi,
                              right_dfi_each_day = scale_data$right_dfi_each_day)

      # impute adfi and get adfi
      adfi_results <- calculate_adfi(dfi_correct = dfi_correct, adg_data = adg_res$adg_data, origin_dfi = prepare_dfi_data$origin_dfi)

      return(adfi_results)

    }, error = function(e) {
      if (!quiet) {
        cli::cli_alert_danger("Error in adfi_get: {e$message}")
      }
      return(NULL)
    }, warning = function(w) {
      if (!quiet) {
        cli::cli_alert_warning("Warning in adfi_get: {w$message}")
      }
    })
  })
}

create_error_types <- function(data, station_type = NULL) {
  responder <- location <- visit_time <- . <- feed_intake <- duration <- weight <- seq_in_day <- seq_days <- 
    frv <- fiv <- otv <- entrancetime <- exittime <- ltd_entrance_step1 <- ftd_exit_step1 <- entrancefeedweight <-
    exitfeedweight <- lwd_exit_step1 <- lwd <- fwd <- ltd <- ftd <- fwd_exit_step1 <- lwd_entrance_step1 <- NULL
  # # Auto-detect station_type
  if (is.null(station_type)) {
    # Check the data structure to determine station_type
    if ("animal_number" %in% names(data) && "state" %in% names(data)) {
      station_type <- "nedap"
      #cli::cli_alert_info("Auto-detected station type: {.field NEDAP}")
    } else if ("entrancetime" %in% names(data) && "exittime" %in% names(data)) {
      station_type <- "fire"
      #cli::cli_alert_info("Auto-detected station type: {.field FIRE}")
    } else {
      cli::cli_abort(c(
        "Cannot auto-detect station type from data",
        "i" = "Please provide 'station_type' parameter or ensure data has the correct columns",
        "i" = "NEDAP data should have `animal_number` and 'state' columns",
        "i" = "FIRE data should have 'entrancetime' and 'exittime' columns"
      ))
    }
  }
  cli::cli_h1("Generating Variables for {.field {toupper(station_type)}} Data")
  cli::cli_process_start("Processing {.field {toupper(station_type)}} station feed intake data")

  # Validate station_type parameter
  valid_types <- c("nedap", "fire")
  if (!station_type %in% valid_types) {
    cli::cli_abort(c(
      "Invalid station_type: {station_type}",
      "i" = "Valid types are: {paste(valid_types, collapse = ', ')}"
    ))
  }

  # Input validation
  if (!inherits(data, "data.table")) {
    data <- data.table::as.data.table(data)
  }

  # Define required columns based on station type
  required_cols <- if (station_type == "nedap") {
    c("responder", "location", "date", "seq_in_location",
      "seq_days", "seq_in_day", "duration", "visit_time",
      "feed_intake", "weight")
  } else {
    c("location", "responder", "date", "seq_in_location",
      "seq_days", "seq_in_day", "entrancetime", "exittime",
      "entrancefeedweight", "exitfeedweight", "feed_intake",
      "weight")
  }

  # Helper function for safe column selection
  select_columns <- function(dt, cols) {
    missing_cols <- setdiff(cols, names(dt))
    if (length(missing_cols) > 0) {
      cli::cli_abort(c(
        "Missing required columns in data",
        "x" = "Missing columns: {.val {missing_cols}}",
        "i" = "Please ensure all required columns are present"
      ))
    }
    dt[, cols, with = FALSE]
  }

  if (station_type == "nedap") {
    # Process Nedap data
    processed_data <- select_columns(data, required_cols)[
      responder != 0 & !is.na(location)
    ][
      order(visit_time), .(
        fiv = feed_intake,
        otv = duration,
        entrancetime = visit_time,
        entrancefeedweight = NA,
        exitfeedweight = NA,
        exittime = visit_time + as.integer(duration),
        weight = weight,
        responder = responder,
        date = date,
        seq_in_day = seq_in_day,
        seq_days = seq_days
      ),
      by = location
    ][, frv := fiv/(otv/60)
    ][, `:=`(
      lwd = NA,
      fwd = NA,
      ltd = NA,
      ftd = NA
    )]
  } else if (station_type == "fire") {
    # Process FIRE data
    processed_data <- select_columns(data, required_cols)
    data.table::setnames(processed_data,
                         "feed_intake", "fiv",
                         skip_absent = TRUE)

    processed_data <- processed_data[
      responder != 0 & !is.na(location)
    ][, entrancetime := do.call(paste, c(.SD, sep = " ")), .SDcol = c("date", "entrancetime")
    ][, exittime := do.call(paste, c(.SD, sep = " ")), .SDcol = c("date", "exittime")
    ][, c("entrancetime", "exittime") := lapply(.SD, lubridate::ymd_hms),
      .SDcols = c("entrancetime", "exittime")
    ][order(entrancetime)
    ][, otv := data.table::fifelse(
      exittime - entrancetime < 0 & lubridate::hour(exittime) == 0,
      exittime - entrancetime + lubridate::ddays(1),
      exittime - entrancetime
    )
    ][, otv := as.numeric(otv)
    ][, frv := fiv / (otv / 60)
    ][, `:=`(
      ltd_entrance_step1 = data.table::shift(entrancetime, type = "lead"),
      ftd_exit_step1 = data.table::shift(exittime, type = "lag")
    )
    ][, `:=`(
      ltd = ltd_entrance_step1 - exittime,
      ftd = entrancetime - ftd_exit_step1
    )
    ][, `:=`(
      lwd_entrance_step1 = data.table::shift(entrancefeedweight, type = "lead"),
      fwd_exit_step1 = data.table::shift(exitfeedweight, type = "lag")
    )
    ][, `:=`(
      lwd = lwd_entrance_step1 - exitfeedweight,
      fwd = entrancefeedweight - fwd_exit_step1
    )
    ][, c("ltd", "ftd") := lapply(.SD, as.numeric),
      .SDcols = c("ltd", "ftd")]
  }

  # Report success
  cli::cli_alert_success("Generated variables for feed intake:")

  if (station_type == "nedap") {
    cli::cli_bullets(c(
      "*" = paste0("{.kbd FIV}", cli::col_blue(" {.emph Feed intake per visit}")),
      "*" = paste0("{.kbd OTV}", cli::col_blue(" {.emph Occupation time per visit}")),
      "*" = paste0("{.kbd FRV}", cli::col_blue(" {.emph Feeding rate per visit}"))
    ))
  } else {
    cli::cli_bullets(c(
      "*" = paste0("{.kbd FIV}", cli::col_blue(" {.emph Feed intake per visit}")),
      "*" = paste0("{.kbd OTV}", cli::col_blue(" {.emph Occupation time per visit}")),
      "*" = paste0("{.kbd FRV}", cli::col_blue(" {.emph Feeding rate per visit}")),
      "*" = paste0("{.kbd LWD}", cli::col_blue(" {.emph Leading weight difference}")),
      "*" = paste0("{.kbd FWD}", cli::col_blue(" {.emph Following weight difference}")),
      "*" = paste0("{.kbd LTD}", cli::col_blue(" {.emph Leading time difference}")),
      "*" = paste0("{.kbd FTD}", cli::col_blue(" {.emph Following time difference}"))
    ))
  }

  # Common processing for both types
  cli::cli_alert_info("Criteria used for identifying errors in feed intake")

  processed_data[, `:=`(
    fiv_lo = data.table::fifelse(fiv < -20, 1, 0),
    fiv_hi = data.table::fifelse(fiv > 2000, 1, 0),
    fiv_0 = data.table::fifelse(otv == 0 & abs(fiv) > 20, 1, 0),
    otv_lo = data.table::fifelse(otv < 0, 1, 0),
    otv_hi = data.table::fifelse(otv > 3600, 1, 0),
    frv_hi_fiv_lo = data.table::fifelse(fiv > 0 & fiv < 50 & frv > 500, 1, 0),
    frv_hi_strict = data.table::fifelse(fiv >= 50 &
                                          any(data.table::shift(fiv, type = "lag") < -20,
                                              data.table::shift(fiv, type = "lead") < -20) &
                                          frv > 110, 1, 0),
    frv_hi = data.table::fifelse(fiv >= 50 &
                                   any(data.table::shift(fiv, type = "lag", n = 2) < -20,
                                       data.table::shift(fiv, type = "lead", n = 2) < -20) &
                                   frv > 170, 1, 0),
    frv_0 = data.table::fifelse(frv == 0 & otv > 500, 1, 0),
    frv_lo = data.table::fifelse(frv != 0 & abs(frv) <= 2, 1, 0),
    lwd_lo = data.table::fifelse((!is.na(lwd)) & lwd < -20, 1, 0),
    lwd_hi = data.table::fifelse((!is.na(lwd)) & lwd > 1800, 1, 0),
    fwd_lo = data.table::fifelse((!is.na(fwd)) & fwd < -20, 1, 0),
    fwd_hi = data.table::fifelse((!is.na(fwd)) & fwd > 1800, 1, 0),
    ltd_lo = data.table::fifelse((!is.na(ltd)) & ltd < 0, 1, 0),
    ftd_lo = data.table::fifelse((!is.na(ftd)) & ftd < 0, 1, 0)
  )]

  # Report error types
  cli::cli_alert_success("Generated error types:")

  if (station_type == "nedap") {
    cli::cli_bullets(c(
      "*" = paste0("{.kbd FIV}", cli::col_blue(" {.emph fiv-lo|fiv-hi|fiv-0}")),
      "*" = paste0("{.kbd OTV}", cli::col_blue(" {.emph otv-lo|otv-hi}")),
      "*" = paste0("{.kbd FRV}", cli::col_blue(" {.emph frv-hi-fiv-lo|frv-hi-strict|frv-hi|frv-0|frv-lo}"))
    ))
  } else {
    cli::cli_bullets(c(
      "*" = paste0("{.kbd FIV}", cli::col_blue(" {.emph fiv-lo|fiv-hi|fiv-0}")),
      "*" = paste0("{.kbd OTV}", cli::col_blue(" {.emph otv-lo|otv-hi}")),
      "*" = paste0("{.kbd FRV}", cli::col_blue(" {.emph frv-hi-fiv-lo|frv-hi-strict|frv-hi|frv-0|frv-lo}")),
      "*" = paste0("{.kbd LWD}", cli::col_blue(" {.emph lwd-lo|lwd-hi}")),
      "*" = paste0("{.kbd FWD}", cli::col_blue(" {.emph fwd-lo|fwd-hi}")),
      "*" = paste0("{.kbd LTD/FTD}", cli::col_blue(" {.emph ltd-lo|ftd-lo}"))
    ))
  }

  cli::cli_process_done()
  return(processed_data)
}
create_error_prop <- function(data) {
  OE <- . <- fiv <- responder <- seq_days <- N <- NULL
  # Define error types with descriptive names
  error_codes <- c(
    fiv_lo = "Feed Intake Volume Low",
    fiv_hi = "Feed Intake Volume High",
    fiv_0 = "Feed Intake Volume Zero",
    otv_lo = "Occupancy Time Low",
    otv_hi = "Occupancy Time High",
    frv_hi_fiv_lo = "Feed Rate High & Intake Low",
    frv_hi_strict = "Feed Rate Critically High",
    frv_hi = "Feed Rate High",
    frv_0 = "Feed Rate Zero",
    frv_lo = "Feed Rate Low",
    lwd_lo = "Last Weight Low",
    lwd_hi = "Last Weight High",
    fwd_lo = "First Weight Low",
    fwd_hi = "First Weight High",
    ltd_lo = "Last Time Low",
    ftd_lo = "First Time Low"
  )

  # Check required columns
  required_cols <- c("date", "seq_in_day", "seq_days", "location",
                     "responder", "weight", "otv", "fiv", "frv",
                     "ltd", "ftd", "lwd", "fwd")
  missing_cols <- setdiff(required_cols, names(data))
  if (length(missing_cols) > 0) {
    cli::cli_abort(c(
      "Missing required columns in input data",
      "x" = "The following columns are missing: {.val {missing_cols}}",
      "i" = "Please ensure all required columns are present in the dataset"
    ))
  }

  cli::cli_h1("Error Properties Data Processing")
  cli::cli_process_start("Processing error properties data")
  #cli::cli_alert_info("Analyzing {.val {length(error_codes)}} error types")

  # Select required columns and calculate total errors
  #cli::cli_process_start("Transforming input data")
  processed_data <- data[, c(required_cols, names(error_codes)), with = FALSE]
  processed_data[, OE := rowSums(.SD, na.rm = TRUE), .SDcols = names(error_codes)]
  #cli::cli_process_done()

  # Calculate original Daily Feed Intake (DFI)
  cli::cli_process_start("Computing original daily feed intake")
  origin_dfi <- processed_data[, .(origin_dfi = sum(fiv)), by = .(responder, seq_days)]
  cli::cli_process_done()

  # Calculate correct DFI part
  cli::cli_process_start("Computing right daily feed intake portion")
  right_dfi <- processed_data[OE == 0, .(dfi_right_part = sum(fiv)), by = .(responder, seq_days)]
  cli::cli_process_done()

  # Extract error DFI data
  cli::cli_process_start("Computing error daily feed intake portion")
  error_dfi_data <- processed_data[OE != 0]
  error_dfi <- error_dfi_data[, .(dfi_error_part = sum(fiv)), by = .(responder, seq_days)]
  cli::cli_process_done()

  # OTD and FID transformation function
  otd_fid_trans <- function(data, target_col, error_cols, new_names) {
    #cli::cli_alert_info("Transforming {.field {toupper(target_col)}} data")

    if (length(error_cols) != length(new_names)) {
      cli::cli_abort(c(
        "Invalid transformation parameters",
        "x" = "Length mismatch: error_cols ({length(error_cols)}) != new_names ({length(new_names)})",
        "i" = "Both vectors must have equal length"
      ))
    }

    temp1 <- function(col_names) {
      tryCatch({
        eval(as.name(data))[eval(as.name(col_names)) > 0,
                            by = .(responder, seq_days),
                            purrr::map(.SD, sum),
                            .SDcols = target_col]
      }, error = function(e) {
        cli::cli_abort(c(
          "Transformation error",
          "x" = "{e$message}",
          "i" = "Verify input data structure and column names"
        ))
      })
    }

    temp2 <- purrr::map(error_cols, temp1)
    temp3 <- purrr::map2(temp2, new_names, function(x, y) data.table::setnames(x, target_col, y))
    temp4 <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), temp3)

    return(temp4)
  }

  # Create transformation parameters table
  useful_list <- data.table::data.table(
    data = c("error_dfi_data", "error_dfi_data"),
    target_col = c("otv", "fiv"),
    error_cols = list(
      I(names(error_dfi_data)[c(14:15, 19:27)]),
      I(names(error_dfi_data)[c(17:18, 28:29)])
    ),
    new_names = list(
      I(paste0("otd_", c(1:2, 6:14))),
      I(paste0("fid_", c(4:5, 15:16)))
    )
  )

  # Apply transformations
  cli::cli_process_start("Executing {.field FID} and {.field OTD} transformations")
  tryCatch({
    otd_fid_result <- purrr::pmap(useful_list, otd_fid_trans)
    cli::cli_alert_success("Transforming {.field FIV} and {.field OTV} to:")
    cli::cli_bullets(c(
      "*" = paste0("{.kbd FID}", cli::col_blue(" {.emph Sum of daily feed intake for error type p}")),
      "*" = paste0("{.kbd OTD}", cli::col_blue(" {.emph Sum of daily occupation time for error type p}"))
    ))

    merged_result <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), otd_fid_result)
  }, error = function(e) {
    cli::cli_abort(c(
      "OTD/FID transformation failed",
      "x" = "{e$message}",
      "i" = "Check transformation parameters and data structure"
    ))
  })
  cli::cli_process_done()

  # Calculate error type proportions
  cli::cli_process_start("Computing error type proportions")
  error_cols <- names(error_codes)
  error_proportions <- processed_data[, c(.N, lapply(.SD, sum)), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, (paste0(error_cols, "_p")) := lapply(.SD, function(x) x / N), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, c(error_cols, "N") := NULL]
  cli::cli_process_done()

  # Merge results
  #cli::cli_progress_step("Merging final results")
  final_result <- merge(merged_result, error_proportions, by = c("responder", "seq_days"), all.x = TRUE)
  final_result <- merge(final_result, error_dfi, by = c("responder", "seq_days"), all.x = TRUE)
  #cli::cli_progress_done()

  # Fill NA values
  # cli::cli_progress_step("Handling missing values")
  # numeric_cols <- names(final_result)[sapply(final_result, is.numeric)]
  # data.table::setnafill(final_result, fill = 0, cols = numeric_cols)
  # cli::cli_progress_done()

  # cli::cli_alert_success(c(
  #   "Data processing completed successfully!",
  #   "v" = "Processed {.val {nrow(final_result)}} rows",
  #   "v" = "Generated {.val {ncol(final_result)}} columns",
  #   "i" = "All error properties have been calculated"
  # ))

  final <- list(
    error_prop_result = final_result,
    origin_dfi = origin_dfi,
    right_dfi = right_dfi,
    error_dfi = error_dfi
  )
  cli::cli_process_done()

  return(final)
}
prepare_scale_data <- function(right_dfi, error_prop, adg_info, adg_data) {
  . <- responder <- seq_days <- dfi_error_part <- dfi_right_part <- location <- lm_slope <- 
    rlm_outliers <- weight <- NULL
  if (any(missing(right_dfi), missing(error_prop), missing(adg_info), missing(adg_data))) {
    cli::cli_alert_warning("Missing required input parameters")
    return(NULL)
  }
  # Check input parameters
  cli::cli_h1("Preparing Data for Feed Intake Calibration")

  cli::cli_process_start("Preparing Data")

  # Extract DFI data for days with errors
  cli::cli_process_start("Processing feed intake with errors")
  right_dfi_in_one_day <- tryCatch({
    error_prop[right_dfi, on = .(responder, seq_days)
    ][!is.na(dfi_error_part)
    ][, dfi_error_part := NULL]
  }, error = function(e) {
    cli::cli_alert_danger("Error while processing data for days with errors: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  # Extract DFI data for days without errors
  cli::cli_process_start("Processing feed intake without errors")
  right_dfi_each_day <- tryCatch({
    error_prop[right_dfi, on = .(responder, seq_days)
    ][is.na(dfi_error_part)
    ][, .(responder, seq_days, dfi_right_part)]
  }, error = function(e) {
    cli::cli_alert_danger("Error while processing data for days without errors: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  # Extract ADG base information
  cli::cli_process_start("Extracting ADG base information")
  base_info_adg <- tryCatch({
    adg_info[, .(responder, location, lm_slope)]
  }, error = function(e) {
    cli::cli_alert_danger("Error while processing ADG information: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  # Extract and calculate average weight for each responder and day
  cli::cli_process_start("Extracting weight information")
  base_info_weight <- tryCatch({
    unique(adg_data[rlm_outliers == FALSE
    ][, weight := as.double(weight)
    ][, .(responder, seq_days, weight)
    ][, weight := mean(weight, na.rm = TRUE),
      by = .(responder, seq_days)][])
  }, error = function(e) {
    cli::cli_alert_danger("Error while processing weight data: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  # Merge datasets
  cli::cli_process_start("Merging all informations")
  merged_data <- tryCatch({
    temp <- merge(right_dfi_in_one_day, base_info_adg,
                  by = "responder", all.x = TRUE)
    temp <- merge(temp, base_info_weight,
                  by = c("responder", "seq_days"), all.x = TRUE)
    temp[!is.na(weight)]
  }, error = function(e) {
    cli::cli_alert_danger("Error while merging datasets: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  # Process data using recipes package
  cli::cli_process_start("Processing data with recipes")
  processed_data <- tryCatch({
    suppressWarnings({  # 使用suppressWarnings包装主要代码
      temp <- data.table::setDF(merged_data) |>
        recipes::recipe(dfi_right_part ~ .) |>
        recipes::update_role(responder, seq_days, location, new_role = "id") |>
        recipes::step_zv(recipes::all_numeric()) |>
        recipes::step_corr(recipes::all_predictors(), threshold = 0.9) |>
        recipes::step_scale(recipes::all_predictors()) |>
        recipes::prep() |>
        recipes::juice()

      temp$responder <- as.factor(temp$responder)

      numeric_cols <- names(temp)[sapply(temp, is.numeric)]
      data.table::setnafill(temp, fill = 0, cols = numeric_cols)
      temp

    })
  }, error = function(e) {
    cli::cli_alert_danger("Error during recipes processing: {e$message}")
    return(NULL)
  })
  cli::cli_process_done()

  #Check processing results
  if (is.null(processed_data) || is.null(right_dfi_each_day)) {
    cli::cli_alert_danger("Data processing failed")
    return(NULL)
  }

  cli::cli_alert_info("Processed data dimensions: {nrow(processed_data)}*{ncol(processed_data)}")

  final <- list(processed_data = processed_data,
                right_dfi_each_day = right_dfi_each_day)

  cli::cli_process_done()

  return(final)
}
train_lmm_model <- function(data, trace = FALSE) {
  fitted <- NULL
  cli::cli_h1("LMM Stepwise Model Selection and Fitting")
  
  predictor_name <- setdiff(names(data), c("responder", "dfi_right_part", "seq_days"))
  if (length(predictor_name) == 0) {
    cli::cli_abort("No predictor variables found in the dataset")
  }
  
  cli::cli_bullets(c("*" = paste0("Selected predictors:\n",
                                  cli::col_blue("{paste(predictor_name, collapse=', ')}"))))
  
  # Create formula
  eq <- reformulate(c(predictor_name, "(1|responder)"), response = "dfi_right_part")
  
  # Try to fit the full model and attempt stepwise selection
  full_model <- NULL
  final_model <- tryCatch({
    # Fit the full model
    full_model <- lme4::lmer(eq, data = data)
    
    # Attempt stepwise selection
    step_res2 <- cAIC4::stepcAIC(
      full_model,
      direction = "backward",
      returnResult = TRUE,
      trace = trace
    )
    
    # Correctly handle the string representation of the formula
    formula_str <- paste(deparse(formula(step_res2$finalModel)), collapse = " ")
    
    cli::cli_bullets(c("*" = paste0("Final model equation:\n",
                                    cli::col_green("{formula_str}"))))
    
    step_res2$finalModel
  }, error = function(e) {
    if (is.null(full_model)) {
      # Full model fitting failed
      cli::cli_alert_danger("Error in initial model fitting: {e$message}")
      return(NULL)
    } else {
      # Full model fitting succeeded, but stepwise failed
      cli::cli_alert_warning("Stepwise selection failed: {e$message}")
      cli::cli_alert_info("Falling back to full model")
      
      # Correctly handle the string representation of the formula
      formula_str <- paste(deparse(formula(full_model)), collapse = " ")
      
      cli::cli_bullets(c("*" = paste0("Using full model equation:\n",
                                      cli::col_yellow("{formula_str}"))))
      return(full_model)
    }
  })
  
  # If model fitting completely failed, return the original data
  if (is.null(final_model)) {
    cli::cli_alert_warning("Model fitting failed, returning original data without predictions")
    return(data)
  }
  
  # Add predicted values
  data_result <- tryCatch({
    data.table::setDT(data)[, fitted := stats::predict(final_model)]
    cli::cli_alert_success("Predictions added to data")
    data
  }, error = function(e) {
    cli::cli_alert_danger("Error in prediction: {e$message}")
    return(data)
  })
  
  return(data_result)
}
merge_lmm_predictions <- function(fit_result, error_dfi, right_dfi_each_day) {
  responder <- seq_days <- NULL
  tryCatch({
    # Stage 4: Data Merging ---------------------------------------------------
    cli::cli_process_start("Merging predictions with error feed intake")
    error_dfi <- merge(fit_result, error_dfi, all.x = TRUE, by = c("responder", "seq_days"))

    temp1 <- error_dfi[, c("responder", "seq_days", "fitted")]
    data.table::setnames(temp1, "fitted", "dfi_right_part")
    cli::cli_process_done()

    # Stage 6: Final Merge ----------------------------------------------------
    cli::cli_process_start("Combining error-free feed intake with corrected portion")
    temp2 <- data.table::rbindlist(list(temp1, right_dfi_each_day))[
      order(responder, seq_days)
    ]

    cli::cli_bullets(c(
      "*" = paste0("Input records: ", cli::col_green("corrected = {nrow(temp1)}, error-free = {nrow(right_dfi_each_day)}")),
      "*" = paste0("Output records: ", cli::col_green("{nrow(temp2)}")) # (+{round(nrow(temp2)/(nrow(temp1)+nrow(right_dfi_each_day))*100,1)}%)
    ))

    cli::cli_process_done()
    # Final Output ------------------------------------------------------------
    return(temp2)

  }, error = function(e) {
    cli::cli_alert_danger("Merging process aborted: {e$message}")
    return(NULL)
  })
}
calculate_adfi <- function(dfi_correct, adg_data, origin_dfi) {
. <- responder <- location <- seq_days <- corrected_dfi_filled <- corrected_dfi <- value_lm <- state <- NULL
  cli::cli_h1("Starting ADFI Calculation Process")
  cli::cli_alert_info("Processing base data...")

  # Base information extraction
  base_info <- tryCatch(
    {
      unique(adg_data[, .(responder, location, date, seq_days)])
    },
    error = function(e) {
      cli::cli_alert_danger("Failed to extract base information: {e$message}")
      stop("Data processing terminated")
    }
  )

  # Data merging
  cli::cli_process_start("Performing data imputation and status labeling")
  correct_dfi <- merge(dfi_correct, base_info, all = TRUE)

  # Column rename validation
  if (!"dfi_right_part" %in% names(correct_dfi)) {
    cli::cli_abort("Cannot find target column 'dfi_right_part' for renaming")
  }
  data.table::setnames(correct_dfi, "dfi_right_part", "corrected_dfi")

  # Main data processing
  all_dfi <- tryCatch(
    {
      all_dfi <- merge(correct_dfi, origin_dfi, all = TRUE)
      all_dfi <- all_dfi[!is.na(location)]
      all_dfi[, corrected_dfi_filled := zoo::na.approx(corrected_dfi, na.rm = FALSE), by = responder]
      all_dfi[, corrected_dfi := data.table::fifelse(is.na(corrected_dfi), corrected_dfi_filled, corrected_dfi)]
      all_dfi[, value_lm := {
        current_responder <- .BY$responder  # # Get the current responder value
        if (all(is.na(corrected_dfi))) {
          cli::cli_alert_warning("Insufficient data for responder {current_responder}, unable to build prediction model")
          corrected_dfi
        } else {
          tryCatch(
            {
              mod <- stats::lm(corrected_dfi ~ seq_days, data = .SD[!is.na(corrected_dfi)])
              data.table::fifelse(is.na(corrected_dfi), stats::predict(mod, newdata = .SD), corrected_dfi)
            },
            error = function(e) {
              cli::cli_alert_danger("Modeling failed for responder {current_responder}: {e$message}")
              rep(NA_real_, .N)
            }
          )
        }
      }, by = responder]
      all_dfi[, corrected_dfi := data.table::fifelse(is.na(corrected_dfi), value_lm, corrected_dfi)]
      all_dfi[, state := data.table::fcase(
        is.na(corrected_dfi_filled) & !is.na(value_lm), "elrp",
        !is.na(origin_dfi) & !is.na(corrected_dfi) & (as.numeric(origin_dfi) == corrected_dfi), "org",
        !is.na(origin_dfi) & !is.na(corrected_dfi) & (as.numeric(origin_dfi) != corrected_dfi), "corr",
        is.na(corrected_dfi) & !is.na(corrected_dfi_filled), "iii",
        default = NA_character_
      )]
      all_dfi <- all_dfi[, .(responder, location, date, seq_days, origin_dfi, corrected_dfi, state)]
      all_dfi
    },
    error = function(e) {
      cli::cli_abort("Data processing pipeline error: {e$message}")
    }
  )

  # Status validation
  if (anyNA(all_dfi$state)) {
    na_count <- sum(is.na(all_dfi$state))
    cli::cli_alert_warning("Found {.val {na_count}} undefined state records")
  }

  # Result aggregation
  cli::cli_alert_info("Generating final results...")
  adfi_info <- all_dfi[, lapply(.SD, mean, na.rm = TRUE),
                       .SDcols = c("origin_dfi", "corrected_dfi"),
                       by = .(responder, location)]

  # Result validation
  if (nrow(adfi_info) == 0) {
    cli::cli_abort("Result aggregation failed, no valid data generated")
  }

  fin <- list(adfi_info = adfi_info, adfi_data = all_dfi)

  cli::cli_alert_info("Final results contain: {.val {nrow(adfi_info)}} responders and {.val {nrow(all_dfi)}} records")
  cli::cli_process_done()
  return(fin)
}
